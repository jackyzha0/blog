<!doctype html><html><head><link rel=preconnect href=https://www.googletagmanager.com><link crossorigin rel=preconnect href=https://www.google-analytics.com><script async src="https://www.googletagmanager.com/gtag/js?id=UA-148413215-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-148413215-1');</script><meta charset=utf-8><title>Machines and Intelligence</title><meta name=description content="jacky zhao blog"><link rel="shortcut icon" type=image/png href=/logo.png><meta name=viewport content="width=device-width,initial-scale=1"><link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&family=Roboto+Mono:wght@700&display=swap" rel=stylesheet><style>:root{--light: #fbfffe;--lightgray: #f0f0f0;--gray: #dddddd;--p: #606060;--dark: #284b63;--visited: #afbfc9;--secondary: #84a59d}.hover{color:var(--dark);text-decoration:none;display:inline-block;position:relative;opacity:.6;z-index:1}.hover::after{transition:300ms;height:20px;content:"";position:absolute;background-color:var(--secondary);opacity:.5;z-index:-1;width:0%;left:15px;bottom:0}header,.singlePost,#postTitle{display:flex;flex-direction:row;align-items:center;justify-content:space-between}#postTitle h1{margin-right:2em}header:hover .hover{opacity:1}header:hover .hover::after{width:100%}body{margin:0;background-color:var(--light)}.desc{flex:1}.desc *{display:inline-block}footer{margin-top:4em;text-align:center}hr{width:20%;margin:3em auto;height:2px;border-radius:1px;border-width:0;color:var(--dark);background-color:var(--dark)}.meta{margin-left:2em!important}#contentWrapper{margin:25px 30vw 25px 25vw}@media all and (max-width:1200px){#contentWrapper{margin:25px 5vw}.scrollIndicator,.sidebarTOC{display:none}}@media all and (max-width:600px){header>nav{display:none}#postTitle{flex-direction:column;align-items:flex-start}}table{width:100%}img{width:100%;border-radius:3px;margin:1em 0}p>img+em{display:block;transform:translateY(-1em)}strong{color:#000}sup{line-height:0}.scrollIndicator{position:fixed;top:25vh;right:25vw;height:50vh;width:2px;background-color:var(--gray)}.sidebarTOC{position:fixed;top:50vh;transform:translateY(-50%);width:200px;height:50vh;right:calc(25vw - 200px)}.sidebarTOC *{list-style:none;margin:auto 0;font-family:Roboto Mono;font-size:.8rem}.sidebarTOC>#TableOfContents{height:100%}.sidebarTOC>#TableOfContents>ul{display:flex;height:100%;flex:1;flex-direction:column}.sidebarTOC li>ul{display:none}.progress{width:2px;background-color:var(--dark)}#newsletter{margin:2em 0}#newsletter>input{padding:.7em 1em;border-radius:4px}#newsletter>input[type=submit]{border:none;color:var(--light);background-color:var(--dark);cursor:pointer}#newsletter>input[type=email]{background-color:var(--light);border:1px solid var(--gray);-webkit-box-sizing:border-box;-moz-box-sizing:border-box;-ms-box-sizing:border-box;box-sizing:border-box}</style><style>.singlePost *{margin:.1em auto}h1,h2,h3,h4,a,.time,ol,ul,thead{font-family:Roboto Mono}a{text-decoration:none;color:var(--dark);transition:all .2s ease}a:hover{color:var(--secondary)!important}.postLink:visited{color:var(--visited)}#posts{list-style:none;padding:0;margin:0}p,tbody,li{font-family:Roboto;color:var(--p);line-height:1.5em}input{font-family:Roboto;color:var(--p)}h2{opacity:.85}h3{opacity:.75}blockquote{margin-left:1em;border-left:3px solid var(--secondary);padding-left:1em}pre{font-family:Roboto Mono;padding:.75em;border-radius:3px;overflow-x:scroll}table{padding:1.5em}td,th{padding:.1em .5em}</style><style>.chroma{color:#272822;background-color:#fafafa}.chroma .err{color:#960050;background-color:#1e0010}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#00a8c8}.chroma .kc{color:#00a8c8}.chroma .kd{color:#00a8c8}.chroma .kn{color:#f92672}.chroma .kp{color:#00a8c8}.chroma .kr{color:#00a8c8}.chroma .kt{color:#00a8c8}.chroma .n{color:#111}.chroma .na{color:#75af00}.chroma .nb{color:#111}.chroma .bp{color:#111}.chroma .nc{color:#75af00}.chroma .no{color:#00a8c8}.chroma .nd{color:#75af00}.chroma .ni{color:#111}.chroma .ne{color:#75af00}.chroma .nf{color:#75af00}.chroma .fm{color:#111}.chroma .nl{color:#111}.chroma .nn{color:#111}.chroma .nx{color:#75af00}.chroma .py{color:#111}.chroma .nt{color:#f92672}.chroma .nv{color:#111}.chroma .vc{color:#111}.chroma .vg{color:#111}.chroma .vi{color:#111}.chroma .vm{color:#111}.chroma .l{color:#ae81ff}.chroma .ld{color:#d88200}.chroma .s{color:#d88200}.chroma .sa{color:#d88200}.chroma .sb{color:#d88200}.chroma .sc{color:#d88200}.chroma .dl{color:#d88200}.chroma .sd{color:#d88200}.chroma .s2{color:#d88200}.chroma .se{color:#8045ff}.chroma .sh{color:#d88200}.chroma .si{color:#d88200}.chroma .sx{color:#d88200}.chroma .sr{color:#d88200}.chroma .s1{color:#d88200}.chroma .ss{color:#d88200}.chroma .m{color:#ae81ff}.chroma .mb{color:#ae81ff}.chroma .mf{color:#ae81ff}.chroma .mh{color:#ae81ff}.chroma .mi{color:#ae81ff}.chroma .il{color:#ae81ff}.chroma .mo{color:#ae81ff}.chroma .o{color:#f92672}.chroma .ow{color:#f92672}.chroma .p{color:#111}.chroma .c{color:#75715e}.chroma .ch{color:#75715e}.chroma .cm{color:#75715e}.chroma .c1{color:#75715e}.chroma .cs{color:#75715e}.chroma .cp{color:#75715e}.chroma .cpf{color:#75715e}.chroma .ge{font-style:italic}.chroma .gs{font-weight:700}</style></head><body><div id=contentWrapper><header><h1><a class=hover href=/>jzhao.xyz</a></h1><nav><a target=_blank href=https://jzhao.xyz>[site]</a>
<a target=_blank href=https://github.com/jackyzha0>[github]</a>
<a target=_blank href=https://twitter.com/_jzhao>[twitter]</a></nav></header><main><article><div id=postTitle class=desc><h1>Machines and Intelligence</h1><p>written November 2, 2020 // 11 min read</p></div><aside class=mainTOC><h3>Table of Contents</h3><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#gofai-systems>GOFAI Systems</a></li><li><a href=#nfai-systems>NFAI Systems</a></li></ul></li><li><a href=#the-potemkin-village-analogy>The Potemkin Village Analogy</a></li><li><a href=#rationality-and-explainability>Rationality and explainability</a></li><li><a href=#models-of-representation>Models of representation</a></li><li><a href=#integrating-gofai-and-nfai>Integrating GOFAI and NFAI</a><ul><li><a href=#what-is-agi>What is AGI?</a></li><li><a href=#dissolving-the-frame-problem>Dissolving the frame problem</a></li><li><a href=#a-non-humanistic-approach>A non-humanistic approach</a></li></ul></li></ul></nav></aside><div class=scrollIndicator><div class=progress id=progressBar></div></div><div class=sidebarTOC><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#gofai-systems>GOFAI Systems</a></li><li><a href=#nfai-systems>NFAI Systems</a></li></ul></li><li><a href=#the-potemkin-village-analogy>The Potemkin Village Analogy</a></li><li><a href=#rationality-and-explainability>Rationality and explainability</a></li><li><a href=#models-of-representation>Models of representation</a></li><li><a href=#integrating-gofai-and-nfai>Integrating GOFAI and NFAI</a><ul><li><a href=#what-is-agi>What is AGI?</a></li><li><a href=#dissolving-the-frame-problem>Dissolving the frame problem</a></li><li><a href=#a-non-humanistic-approach>A non-humanistic approach</a></li></ul></li></ul></nav></div><div id=mainText><p>This blog post is adapted from a term paper I wrote for PHIL250: Minds and Machines at UBC. I hope you enjoy the post and learn as much as I did in writing it!</p><hr><h2 id=introduction>Introduction</h2><p>Historically, development of AI has taken a very specific approach — systems that represent the world through symbols and manipulate those tokens in a systematic way to arrive at a result. This type of AI was coined Good Old-Fashioned AI (GOFAI) by John Haugeland<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>This worked well up until around 1984 when the field entered an &lsquo;AI Winter&rsquo;, a long plateau in progress that was most likely due cynicism in the AI research community that trickled to media and funding bodies, halting research and development<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>However, with the rise of Moore&rsquo;s Law and the insane amount of compute and data available, a new approach to the development of AI arose — one that focused on statistical methods and connectionist networks like artificial neural networks<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. Haugeland<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> dubbed this approach to AI design New Fangled AI (NFAI).</p><p>This paper will examine factors that differentiate GOFAI and NFAI systems, such as their ability to adapt to changes in input, and the explainability of their outputs and internal representations. It will also examine current work in integrating the two approaches to Artificial Intelligence to create an artificial general intelligence.</p><h3 id=gofai-systems>GOFAI Systems</h3><p>Since the inception of the term GOFAI, the basic idea has remained unchanged: thinking as internal symbol manipulation. Within these GOFAI systems, symbols are representative of aspects of our world. These symbols are manipulated in a systematic and logical matter, performing a series of deterministic steps that results in another sequence of symbols<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>A very common example of GOFAI systems are expert systems, which are computer systems that emulate the decision making ability of a human expert<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. They solve problems via decision-tree reasoning, figuring out whether to perform certain actions based off of if-then rules.</p><p>However, just being able to solve a problem shouldn&rsquo;t be sufficient for intelligence. So what qualifies it? At its core, GOFAI can be considered &lsquo;artificially intelligent&rsquo; because of semantic interpretation. If the symbols represent aspects of our world, the result, which is also a symbol sequence, can be <em>translated</em> back into aspects of our world. This is called semantic interpretation, which &ldquo;seeks to construe a body of symbols so that what they mean (&lsquo;say&rsquo;) turns out to be consistently reasonable and sensible, given the situation&rdquo;<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><h3 id=nfai-systems>NFAI Systems</h3><p>NFAI, on the other hand, is a diverse and still rapidly evolving set of systems and algorithms. It is more of a grab-bag term, roughly meaning any sort of scientific mind design that is not GOFAI<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. Under this umbrella are connectionist networks, which are networks composed of lots of simple units that are interconnected with various strengths. This paper will mostly focus on connectionism as a synecdoche for the greater umbrella of NFAI.</p><p>Some classic examples of connectionist networks include convolutional neural networks (CNNs), which are a form of image classifiers<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. These networks operate by applying filters or kernels to an input between layers of the network. Each of those filters have their own set of strengths that will learn and evolve over time to identify certain &lsquo;features&rsquo; from the input. Similar to cell assemblies in animal perceptual systems, these filters assemble more complex patterns using smaller and simpler patterns<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>.</p><p>These connectionist networks are very inspired by the structure of the brain, with its hierarchical patterns and compositional nature<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, rather than the rational manipulation of symbols that is observed in GOFAI.</p><h2 id=the-potemkin-village-analogy>The Potemkin Village Analogy</h2><p>While it is obvious that GOFAI and NFAI are very different approaches to constructing AI systems, how do they differ in their resilience to failure? An analogy that may be useful in visualizing this is a potemkin village — a fake village that is built to resemble and deceive others into thinking it is real. AI systems attempt to build a sort of &lsquo;potemkin village&rsquo; that &ldquo;works well on naturally occurring data, but is exposed as fake when one visits points in space that do not have high probability&rdquo;<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>.</p><p>GOFAI systems are excellent at &ldquo;processing syntactical patterns like those characteristic of logical formulae, ordinary sentences, and many inferences&rdquo;<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, but are also very narrow-minded and vulnerable when it comes to unexpected variations or oddities in the input given. The potemkin village that a GOFAI system may construct will hold up if only seen from the intended angles, but any slight deviation from an intended or expected input would shatter the illusion immediately.</p><p>NFAI systems, on the other hand, are &ldquo;adept at finding various sort of similarities among patterns, at recognizing repeated (or almost repeated) patterns and filling in missing parts of incomplete patterns&rdquo;<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. These also happen to be the exact things that GOFAI systems struggle with. The potemkin village that a NFAI system may construct will hold up much more robustly to unexpected patterns or noisy input, but will, at heart, still be a fake village.</p><h2 id=rationality-and-explainability>Rationality and explainability</h2><p>In GOFAI systems, intentionality — the meaning and semantics behind the tokens — is injected through explicit programming by those who create it. These GOFAI systems are able to process these tokens and make conclusions based off of logic and reason rather than just trial-and-error. Case in point, expert systems. These if-then statements can easily explain decisions by showing which parts evaluated as true or false in its decision making process<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><p>Connectionist systems, for the most part, are very hard to explain and are often dubbed black-box models due to the hidden nature of its internal workings. Unlike GOFAI systems, its internal representation model is defined by the state of the entire network rather than that of any single unit — this is commonly referred to as a distributed model of connectionist representation<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup> and is often claimed to be one of the distinctive features of connectionism.</p><h2 id=models-of-representation>Models of representation</h2><p>To put it in sound terminology, note while in the GOFAI system, the <em>tokens</em> are the objects of formal processing, so the system which manipulates the tokens is the actual vehicle of computation. The tokens themselves are also <em>representations</em> of aspects of the world, so they are also vehicles of mental content. In GOFAI systems, tokens are both the vehicle of computation and the vehicle of mental content.</p><p>This is in contrast with connectionist systems, where computation is performed at the level of simple units (unit activations, backpropagation), meaning the units are the vehicles of computation. However, as these systems use a distributed model of representation, it is not a single unit that represents something, but rather the &ldquo;network state as a whole thats interpreted as representing&rdquo;<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>. Thus, in connectionist systems, the vehicles of computation (units) need to be the vehicles of representation (network state).</p><h2 id=integrating-gofai-and-nfai>Integrating GOFAI and NFAI</h2><p>Given that GOFAI and NFAI systems seem so vastly different in their approaches to AI, how might one go about reconciling them?</p><p>One approach is to combine both into one system. This is used when there’s a rational, known, and algorithmic way to process a subproblem. Systems like AlphaZero, a connectionist based Go playing system, use mixed systems to achieve the level of performance they report. Although at heart, AlphaZero uses a deep neural network to assess new positions, it also uses a Monte Carlo Tree Search (a GOFAI algorithm) to determine its next move based of the assessment of the neural net<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>.</p><p>Another, less researched method, are interpretable connectionist systems. As traditional connectionist networks rely on the network state being the vehicle of representation, the complexity, depth, and scale of modern connectionist models means that it is becoming increasingly difficult for humans to interpret the output. The field of explainable AI (XAI) focuses on incentivizing connectionist networks to develop localist representations (i.e. moving away from having the vehicle of representation be at the network level, but at the unit level). Zhang, Wu, and Zhu of UCLA recently showed that it is possible to train a CNN to use &lsquo;interpretable filters&rsquo;, which encourage networks to group feature detectors into single filters, showing the possibility of moving from distributed representations to more local representations<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>.</p><h3 id=what-is-agi>What is AGI?</h3><p>While intelligence can be understood in many ways, this paper will focus on examining the prospects of emulating or achieving the capacity to understand or learn anything a human can — the hallmark of an artificial general intelligence (AGI).</p><p>Most commentators would agree that current AI systems fall short of implementing general intelligence<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. These are narrow AI systems, which are used to accomplish or solve specific tasks like the game of Go or language translation, rather than to attempt to create a system capable of AGI. So, what&rsquo;s stopping us from making the transition from domain-specific algorithms to domain-general algorithms?</p><p>One problem that stumped earlier attempts at AGI was the <em>common-sense problem</em>: how do we represent common-sense information that is obvious to most humans in a way that is accessible to AI systems that use natural language? Unsurprisingly, the problem of storing all of this information was solved by the massive explosion in compute and data in the past few decades<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. However, the difficult part of this problem, choosing what subset of that huge information bank is relevant in any situation, remains a huge unsolved problem. How do we update our database of knowledge when relationships between symbols change? This is referred to as the frame problem.</p><h3 id=dissolving-the-frame-problem>Dissolving the frame problem</h3><p>Dreyfus<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> posits that any AI systems which attempt to tackle the frame problem through storing relevant frames are bound to failure. He argues that, &ldquo;human beings do not simply store common-sense information,&rdquo; rather they &ldquo;directly perceive and act upon significance in their environment&rdquo;. In his view, a more Heideggerian approach to AI will dissolve this problem.</p><p>Heideggerian AI, in its most basic sense, is concerned with
the Heideggerian concept of Dasein, which literally means &lsquo;Being-there&rsquo;<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>. Through the use of this expression, Heidegger calls to attention the fact that a human cannot exist or be taken into account without existing in context of a world with other things — &ldquo;to be human is to be fixed, embedded, and immersed in the physical, literal, tangible day to day world&rdquo;<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>.</p><p>Dreyfus believed that, for any AI system to achieve any sort of general intelligence, it must also exhibit Dasein. Thus, &ldquo;a successful Heideggerian AI would need a perfect model of the human body – and by implication, that Dasein must be expressed as a human being, organically as well as existentially&rdquo;<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>.</p><h3 id=a-non-humanistic-approach>A non-humanistic approach</h3><p>However, Steed refutes Dreyfus&rsquo; overly humanistic interpretation of Heideggerian AI, believing that a AI model only needs to be &ldquo;embedded and embodied such that what AI experiences is significant for AI in the particular way that AI is,&rdquo; and thus intelligence would be possible by Heideggerian standards<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>.</p><p>The refutation against a purely anthropocentric view of AI brings to light an important concept: the multiple realization argument. Emulating or copying human intelligence isn&rsquo;t the only way to achieve intelligence that rivals that of humans.</p><p>Contemporary AI systems are almost always used as a problem solving tool, a means to tackle uniquely human problems and to convey results that are semantically useful to us. As a result, these approaches are doomed to be constrained by human problems. However, if we look outside the anthropocentric view of intelligence, AI systems may not share these human problems with us and &ldquo;perhaps an authentic, free AI system does not converge to a solution that is interpretable from a human standpoint at all&rdquo;<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>.</p><p>AI is already capable of learning, adaptation, and basic Being-in-the-world. Thus, to achieve general intelligence, we should allow AI to contemplate its own problems and existence.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Huageland, John. (1996). <em>What Is Mind Design?</em> Mind Design II, doi:10.7551/mitpress/4626.003.0001. <a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2 role=doc-endnote><p>Hendler, J. (2008). <em>Avoiding another AI winter.</em> IEEE Intelligent Systems, (2), pp. 2-4. <a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3 role=doc-endnote><p>Jackson, Peter (1998). <em>Introduction To Expert Systems</em> (3 ed.). Addison Wesley. p. 2. ISBN 978-0-201-87686-4. <a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4 role=doc-endnote><p>Buckner, C. (2019). <em>Deep learning: A philosophical introduction.</em> Philosophy Compass, 14(10), e12625. <a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5 role=doc-endnote><p>Zhang, Q., Nian Wu, Y., & Zhu, S. C. (2018). <em>Interpretable convolutional neural networks.</em> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 8827-8836). <a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6 role=doc-endnote><p>Churchland, P. (1990). <em>Thinking: An invitation to cognitive science.</em> Vol. 3., pp. 199-228. <a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7 role=doc-endnote><p>Goodfellow, I., Shlens, J., & Szegedy, C. (2014) <em>Explaining and harnessing adversarial examples.</em> ArXiv Preprint ArXiv: 1412.6572. <a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8 role=doc-endnote><p>Crane, Tim. (2003). <em>The Mechanical Mind.</em> doi:10.4324/9780203426319. <a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9 role=doc-endnote><p>Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., &mldr; & Lillicrap, T. (2017). <em>Mastering chess and shogi by self-play with a general reinforcement learning algorithm.</em> arXiv preprint arXiv:1712.01815. <a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10 role=doc-endnote><p>Dreyfus, Hubert L. (2008) <em>Why Heideggerian AI Failed and How Fixing It Would Require Making It More Heideggerian.</em> The Mechanical Mind in History, pp. 331–362., doi:10.7551/mitpress/9780262083775.003.0014. <a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11 role=doc-endnote><p>Solomon, R. (1972), <em>From Rationalism to Existentialism: The Existentialists and Their Nineteenth Century Backgrounds</em>, Harper & Row, New York. <a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12 role=doc-endnote><p>Steiner, G. (1978), <em>Heidegger</em>, The Harvester Press Limited, Sussex <a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13 role=doc-endnote><p>Steed, R. (2019). <em>AI is Heideggerian Enough, But Can It Be Authentic?</em> Unpublished manuscript, Carnegie Mellon. <a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><div><hr><p>I'm always happy to have follow up conversations or hear your thoughts! Send me an email at <a href=mailto:j.zhao2k19@gmail.com>j.zhao2k19@gmail.com</a>. If you like this content and want to keep up to
date with my latest ramblings and whatever else I've found interesting on the web, you should consider subscribing to my
newsletter too :)</p><form id=newsletter action=https://buttondown.email/api/emails/embed-subscribe/jacky method=post target=popupwindow onsubmit="window.open('https://buttondown.email/jacky','popupwindow')" class=embeddable-buttondown-form><input type=email placeholder=name@email.com name=email id=bd-email>
<input type=hidden value=1 name=embed>
<input type=submit value=Subscribe></form><a href=/>← (country roads) take me home</a></div></article><script>window.onscroll=()=>{const scrollAmt=document.body.scrollTop||document.documentElement.scrollTop
const height=document.documentElement.scrollHeight-document.documentElement.clientHeight
const scrolled=(scrollAmt/height)*100
document.getElementById("progressBar").style.height=scrolled+"%"}</script></main><footer><p>made by jacky zhao, © 2020</p><a href=https://github.com/jackyzha0/blog target=_blank>[source]</a></footer></div></body></html>