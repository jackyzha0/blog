<!doctype html><html><head><link rel=preconnect href=https://www.googletagmanager.com><link crossorigin rel=preconnect href=https://www.google-analytics.com><script async src="https://www.googletagmanager.com/gtag/js?id=UA-148413215-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-148413215-1');</script><meta charset=utf-8><meta name=description content="jacky zhao personal portfolio - software developer trying to build solutions to the world’s problems"><meta name=viewport content="width=device-width,initial-scale=1"><link href="https://fonts.googleapis.com/css2?family=Roboto&family=Roboto+Mono:wght@700&display=swap" rel=stylesheet><style>:root{--light: #fbfffe;--lightgray: #f0f0f0;--gray: #dddddd;--p: #606060;--dark: #284b63;--secondary: #84a59d}.hover{color:var(--dark);text-decoration:none;display:inline-block;position:relative;opacity:.6;z-index:1}.hover::after{transition:300ms;height:20px;content:"";position:absolute;background-color:var(--secondary);opacity:.5;z-index:-1;width:0%;left:15px;bottom:0}header,.singlePost,#postTitle{display:flex;flex-direction:row;align-items:center;justify-content:space-between}#postTitle h1{margin-right:4em}header:hover .hover{opacity:1}header:hover .hover::after{width:100%}body{margin:0;background-color:var(--light)}.desc{flex:1}.desc *{display:inline-block}footer{margin-top:4em;text-align:center}.meta{margin-left:2em!important}#contentWrapper{margin:25px 25vw}@media all and (max-width:900px){#contentWrapper{margin:25px 5vw}}@media all and (max-width:600px){nav{display:none}}table{width:100%}img{width:100%;border-radius:3px;margin:2em 0}strong{color:#000}</style><style>.singlePost *{margin:.1em auto}h1,h2,h3,h4,a,.time,ol,ul,thead{font-family:Roboto Mono}a{text-decoration:none;color:var(--dark);transition:all .2s ease}#posts{list-style:none;padding:0;margin:0}p,tbody{font-family:Roboto;color:var(--p);line-height:1.5em}h2{opacity:.85}h3{opacity:.75}blockquote{margin-left:1em;border-left:3px solid var(--secondary);padding-left:1em}pre{font-family:Roboto Mono;padding:.75em;border-radius:3px}table{padding:1.5em}td,th{padding:.1em .5em}</style><style>.chroma{color:#272822;background-color:#fafafa}.chroma .err{color:#960050;background-color:#1e0010}.chroma .lntd{vertical-align:top;padding:0;margin:0;border:0}.chroma .lntable{border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block}.chroma .hl{display:block;width:100%;background-color:#ffc}.chroma .lnt{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .ln{margin-right:.4em;padding:0 .4em;color:#7f7f7f}.chroma .k{color:#00a8c8}.chroma .kc{color:#00a8c8}.chroma .kd{color:#00a8c8}.chroma .kn{color:#f92672}.chroma .kp{color:#00a8c8}.chroma .kr{color:#00a8c8}.chroma .kt{color:#00a8c8}.chroma .n{color:#111}.chroma .na{color:#75af00}.chroma .nb{color:#111}.chroma .bp{color:#111}.chroma .nc{color:#75af00}.chroma .no{color:#00a8c8}.chroma .nd{color:#75af00}.chroma .ni{color:#111}.chroma .ne{color:#75af00}.chroma .nf{color:#75af00}.chroma .fm{color:#111}.chroma .nl{color:#111}.chroma .nn{color:#111}.chroma .nx{color:#75af00}.chroma .py{color:#111}.chroma .nt{color:#f92672}.chroma .nv{color:#111}.chroma .vc{color:#111}.chroma .vg{color:#111}.chroma .vi{color:#111}.chroma .vm{color:#111}.chroma .l{color:#ae81ff}.chroma .ld{color:#d88200}.chroma .s{color:#d88200}.chroma .sa{color:#d88200}.chroma .sb{color:#d88200}.chroma .sc{color:#d88200}.chroma .dl{color:#d88200}.chroma .sd{color:#d88200}.chroma .s2{color:#d88200}.chroma .se{color:#8045ff}.chroma .sh{color:#d88200}.chroma .si{color:#d88200}.chroma .sx{color:#d88200}.chroma .sr{color:#d88200}.chroma .s1{color:#d88200}.chroma .ss{color:#d88200}.chroma .m{color:#ae81ff}.chroma .mb{color:#ae81ff}.chroma .mf{color:#ae81ff}.chroma .mh{color:#ae81ff}.chroma .mi{color:#ae81ff}.chroma .il{color:#ae81ff}.chroma .mo{color:#ae81ff}.chroma .o{color:#f92672}.chroma .ow{color:#f92672}.chroma .p{color:#111}.chroma .c{color:#75715e}.chroma .ch{color:#75715e}.chroma .cm{color:#75715e}.chroma .c1{color:#75715e}.chroma .cs{color:#75715e}.chroma .cp{color:#75715e}.chroma .cpf{color:#75715e}.chroma .ge{font-style:italic}.chroma .gs{font-weight:700}</style></head><body><div id=contentWrapper><header><h1><a class=hover href=/>jzhao.xyz</a></h1><nav><a target=_blank href=https://jzhao.xyz>[site]</a>
<a target=_blank href=https://github.com/jackyzha0>[github]</a>
<a target=_blank href=https://twitter.com/_jzhao>[twitter]</a></nav></header><main><article><div id=postTitle class=desc><h1>reflect: NLP model explained pt.2</h1><p>written May 10, 2020 // 1907 words</p></div><div id=mainText><p>How do we tell that this is a “valid” intent?</p><p>In this second part, we’ll take a deep dive into the neural network that helps us solve this hefty problem of classifying whether an intent is valid or not.</p><p>Recall that in part 1, we went over the data we had, and how we could prepare that data to feed into our neural network. Now that we have all our data ready and prepped, let’s talk about the model!</p><h2 id=the-model>The model</h2><p>The type of neural network that we’ll be using is called Long Short-Term Memory (LSTM).</p><p><img src=https://cdn-images-1.medium.com/max/4000/0*HyoZq6fOfsnn2YOC alt="credit: Christopher Olah, 2015"><em>credit: Christopher Olah, 2015</em></p><p>What’s so special about these networks is that they are really good at modelling time-series data, making it an ideal candidate for tasks like forecasting or natural language processing.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># Function to create a RNN model with given parameters</span>
<span class=c1># max_seq_len: maximum token sequence length</span>
<span class=c1># vocab_size:  size of tokenizer vocabulary</span>
<span class=k>def</span> <span class=nf>RNN</span><span class=p>(</span><span class=n>max_seq_len</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span><span class=p>:</span>
    <span class=n>inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>inputs</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>max_seq_len</span><span class=p>]</span><span class=p>)</span>
    <span class=n>layer</span> <span class=o>=</span> <span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>input_length</span><span class=o>=</span><span class=n>max_seq_len</span><span class=p>)</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
    <span class=n>layer</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>return_sequences</span> <span class=o>=</span> <span class=bp>True</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
    <span class=n>layer</span> <span class=o>=</span> <span class=n>Dropout</span><span class=p>(</span><span class=mf>0.5</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
    <span class=n>layer</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
    <span class=n>layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>FC1</span><span class=s1>&#39;</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
    <span class=n>layer</span> <span class=o>=</span> <span class=n>Dropout</span><span class=p>(</span><span class=mf>0.5</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
    <span class=n>layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>out_layer</span><span class=s1>&#39;</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
    <span class=n>layer</span> <span class=o>=</span> <span class=n>Activation</span><span class=p>(</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>sigmoid</span><span class=s1>&#39;</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>(</span><span class=n>inputs</span><span class=o>=</span><span class=n>inputs</span><span class=p>,</span> <span class=n>outputs</span><span class=o>=</span><span class=n>layer</span><span class=p>)</span>
    <span class=k>return</span> <span class=n>model</span>
</code></pre></div><p>Here’s how we define it in our <a href=https://github.com/jackyzha0/reflect-nlp/blob/master/nlp/net.py>Keras code</a>, don’t worry if you don’t understand it just yet! We’ll explain it in the next few paragraphs.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>inputs</span> <span class=o>=</span> <span class=n>Input</span><span class=p>(</span><span class=n>name</span><span class=o>=</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>inputs</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>max_seq_len</span><span class=p>]</span><span class=p>)</span>
</code></pre></div><p>Right off the bat, you’ll notice that the first layer is the Input layer. Basically, this tells Keras to instantiate a new tensor (a multi-dimensional vector) with a given shape. In this case, we’re creating a one-dimensional tensor that is max_seq_len units long. When we trained our model, this was set to 75.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>layer</span> <span class=o>=</span> <span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=mi>64</span><span class=p>,</span> <span class=n>input_length</span><span class=o>=</span><span class=n>max_seq_len</span><span class=p>)</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</code></pre></div><p>Next up, we have the Embedding layer. We could get into a really technical discussion about what this really does, but you can think of it as a layer that helps the neural network to learn semantic relationships between inputs.</p><p><img src=https://cdn-images-1.medium.com/max/3010/0*YOZ_CfmtgpUbJ9BD alt="credit: Rutger Ruizendaal, 2017"><em>credit: Rutger Ruizendaal, 2017</em></p><p>Essentially, it embeds tokens in a higher dimension vector space, where distance between tokens represents its similarity.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>layer</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>return_sequences</span> <span class=o>=</span> <span class=bp>True</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
</code></pre></div><p>Now, we get into the meat of the neural network: the LSTM layer. As stated before, these LSTM networks are really good at modelling time series data like language. In this case, our LSTM network has 64 hidden units per cell, and that we’d like to pass these hidden states to the next layer. If you’d like to learn more about the inner workings of the LSTM model, theres a really good resource <a href=https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21>here</a>!</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>layer</span> <span class=o>=</span> <span class=n>Dropout</span><span class=p>(</span><span class=mf>0.5</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
</code></pre></div><p>Next, you’ll notice there are a few Dropout layers. These layers help to prevent overfitting by randomly killing off connections between the two layers (a sort of regularization). This makes sure neurons aren’t just “memorizing” the input data. This is especially important because our dataset is relatively small (~2000 observations even after augmentation), so making sure that our machine learning model can generalize outside of this limited dataset is really important.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>layer</span> <span class=o>=</span> <span class=n>LSTM</span><span class=p>(</span><span class=mi>64</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
</code></pre></div><p>We have yet another LSTM layer! By having these two chained right after each other, the first layer can pass all the values of all of its hidden states to the second layer, effectively allowing a sort of ‘deeper’ neural network.</p><p><img src=https://cdn-images-1.medium.com/max/2000/0*0BRdnA5sJBYbaeR9 alt="credit: Jianjing Zhang 2018"><em>credit: Jianjing Zhang 2018</em></p><p>This deep LSTM allows our network to learn more abstract concepts, making them well suited for natural language tasks.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>FC1</span><span class=s1>&#39;</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
</code></pre></div><p>Next, we have something called a Fully Connected layer, or a Dense layer. In a dense layer, each of the input neurons is connected to every output neuron. This kind of ‘glue’ layer helps the network to pick out and discriminate feature output by our previous LSTM layer.</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=n>layer</span> <span class=o>=</span> <span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>out_layer</span><span class=s1>&#39;</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
<span class=n>layer</span> <span class=o>=</span> <span class=n>Activation</span><span class=p>(</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>sigmoid</span><span class=s1>&#39;</span><span class=p>)</span><span class=p>(</span><span class=n>layer</span><span class=p>)</span>
</code></pre></div><p>Similarly, we have one final Dense layer that ‘compresses’ all of the hidden units down to one neuron. However, we want the output value of this neuron to be how confident from a scale of 0 to 1 it is that the intent is valid. We do this by applying something called an activation function.</p><p><img src=https://cdn-images-1.medium.com/max/2000/0*kdowh3GOGOUvGBr0 alt="A sigmoid activation function"></p><p>In this case, the particular function we chose is the sigmoid activation function, which looks something like the above.</p><h3 id=model-overview>Model Overview</h3><p>Phew, finally got through everything! After putting it all together, we end up with a network that looks something like this:</p><div class=highlight><pre class=chroma><code class=language-python data-lang=python><span class=c1># 75 max_seq_len</span>
<span class=c1># 1000 tokenizer_vocab_size</span>
<span class=n>model</span> <span class=o>=</span> <span class=n>net</span><span class=o>.</span><span class=n>RNN</span><span class=p>(</span><span class=mi>75</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span> 
<span class=n>model</span><span class=o>.</span><span class=n>summary</span><span class=p>(</span><span class=p>)</span>

<span class=c1># _________________________________________________________________</span>
<span class=c1># Layer (type)                 Output Shape              Param #   </span>
<span class=c1># =================================================================</span>
<span class=c1># inputs (InputLayer)          (None, 75)                0         </span>
<span class=c1># _________________________________________________________________</span>
<span class=c1># embedding_1 (Embedding)      (None, 75, 64)            64000     </span>
<span class=c1># _________________________________________________________________</span>
<span class=c1># lstm_1 (LSTM)                (None, 75, 64)            33024     </span>
<span class=c1># _________________________________________________________________</span>
<span class=c1># dropout_1 (Dropout)          (None, 75, 64)            0         </span>
<span class=c1># _________________________________________________________________</span>
<span class=c1># lstm_2 (LSTM)                (None, 64)                33024     </span>
<span class=c1># _________________________________________________________________</span>
<span class=c1># FC1 (Dense)                  (None, 256)               16640     </span>
<span class=c1># _________________________________________________________________</span>
<span class=c1># dropout_2 (Dropout)          (None, 256)               0         </span>
<span class=c1># _________________________________________________________________</span>
<span class=c1># out_layer (Dense)            (None, 1)                 257       </span>
<span class=c1># _________________________________________________________________</span>
<span class=c1># activation_1 (Activation)    (None, 1)                 0         </span>
<span class=c1># =================================================================</span>
<span class=c1># Total params: 146,945</span>
<span class=c1># Trainable params: 146,945</span>
<span class=c1># Non-trainable params: 0</span>
<span class=c1># _________________________________________________________________</span>
</code></pre></div><p>Theres a grand total of 150,000 different trainable knobs and parameters in our neural network!</p><h2 id=training-pipeline>Training pipeline</h2><p>So, how does the data we got earlier play a role in helping our machine learning model learn and improve?</p><p>The first component is the <strong>loss function.</strong> This component tells the neural network how ‘correct’ its prediction was. In this model, we will use something called <a href=https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a>binary cross entropy</a>, which is basically a fancy word for log-based error. If the true label is 1, we can then show what the log-loss would be for some given prediction probability.</p><p><img src=https://cdn-images-1.medium.com/max/2000/0*8rd4ho_3Y6zrtra-.png alt></p><p>Next, we need to pick an <strong>optimizer</strong>. This component tells the neural network how to change its parameters to improve or ‘optimize’ itself. In this model, we chose to use an optimizer called <a href=https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a>RMSProp</a> with a learning rate of 1e-3 . We aren’t going to cover all the technical details of this optimizer in this blog post, but just know that it is a very fast and effective optimizer.</p><p><img src=https://cdn-images-1.medium.com/max/2000/0*HZM5XJ-quu276w39.gif alt="RMSProp(black) vs a bunch of other optimizers. credit: Vitaly Bushaev"><em>RMSProp(black) vs a bunch of other optimizers. credit: Vitaly Bushaev</em></p><p>One important hyperparameter we choose is the <strong>train-test split.</strong> In data science and machine learning, we typically withhold part of our data and set it aside as a <strong>test set</strong>. The rest of the data will be considered the <strong>training set.</strong> When training the model, we never feed it the test set. As a result, we can use the test set as a metric to see how well it would perform on real-world, unseen data. In our training, we used a train-test split of 20%.</p><p>Another important hyperparameter that we can choose is the <strong>mini-batch size</strong>. The mini-batch size determines how many training examples we feed the machine learning model before updating its parameters. A smaller mini-batch means that we get more frequent updates to the parameters, but it also runs the risk of having outliers that may cause a bad gradient update. A large mini-batch means that we get a more accurate gradient update but it also takes longer. A similar concept is <em>sample size</em> in statistics. We could pick a larger sample to get a better estimate of the overall population, but it is often more expensive to do so. A smaller sample might contain outliers and thus be less robust of an estimate of the overall population, but it very easy to do. So, there’s this tradeoff between accuracy and speed. We found that a good balance between these was a mini-batch size of 128.</p><p>We then trained our neural network over 10 epochs. A single epoch is one iteration over the entire dataset. If we train it for too many epochs, you run the risk of overfitting (memorizing the training data), but we don’t train it enough, we run the risk of not discovering a better model. One thing we can do it minimize this problem is through the use of cross-validation, which is a technique that lets us ‘test’ on portions of the training set. Essentially, at each iteration during training, we withhold a portion of the training set and use it as a sort of ‘validation’.</p><p><img src=https://cdn-images-1.medium.com/max/2000/0*6XoMgZUd3SxXgqBj.png alt="credit: Raheel Shaikh"><em>credit: Raheel Shaikh</em></p><p>By seeing when this validation accuracy goes down, we can get a pretty good idea of when our model begins to overfit on our data, and stop the training before this happens. In training our model, we will use 5-fold <a href=https://towardsdatascience.com/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85>cross-validation</a>.</p><p>After all of this, we end with a training accuracy of 93.60% and test accuracy of 85.95%. Not bad at all!</p><h2 id=serving-the-model>Serving the model</h2><p>Great! So now we have a trained model. Can we put that in the Chrome extension now? Not quite yet…</p><p>Our model was written and trained with Keras (a Python Deep Learning library). Our Chrome extension is written in TypeScript. How do we get these two to work together?</p><p>Luckily for us, Tensorflow.js exists! This library allows us to run Tensorflow models from within JavaScript. Tensorflow has released a script that lets us to convert a Keras model into something that Tensorflow.js understands, so we can run that to convert our models.</p><p>However, we can’t just directly plug-and-play. You may remember that we did all of that data preprocessing before we trained our model. Tensorflow.js doesn’t have any of this built in, so we made our own implementation of it. You can check it out <a href=https://github.com/jackyzha0/reflect-chrome/blob/master/src/nn.ts>here</a>.</p><p>We’ll leave all the technical code out (if you’re interested, feel free to peek around the source code!), but we’ve abstracted it enough that classifying an intent is a breeze.</p><div class=highlight><pre class=chroma><code class=language-typescript data-lang=typescript><span class=c1>// declared somewhere earlier
</span><span class=c1></span><span class=kr>const</span> <span class=nx>model</span>: <span class=kt>nn.IntentClassifier</span> <span class=o>=</span> <span class=k>new</span> <span class=nx>nn</span><span class=p>.</span><span class=nx>IntentClassifier</span><span class=p>(</span><span class=s2>&#34;acc85.95&#34;</span><span class=p>)</span><span class=p>;</span> <span class=c1>// name of converted model
</span><span class=c1></span>
<span class=c1>// send to nlp model for prediction
</span><span class=c1></span><span class=kr>const</span> <span class=nx>valid</span>: <span class=kt>boolean</span> <span class=o>=</span> <span class=nx>await</span> <span class=nx>model</span><span class=p>.</span><span class=nx>predict</span><span class=p>(</span><span class=nx>intent</span><span class=p>)</span><span class=p>;</span>
<span class=k>if</span> <span class=p>(</span><span class=nx>valid</span><span class=p>)</span> <span class=p>{</span>
    <span class=c1>// let through
</span><span class=c1></span><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
    <span class=c1>// block page
</span><span class=c1></span><span class=p>}</span>
</code></pre></div><h2 id=future-improvement>Future improvement</h2><h3 id=possible-models>Possible models</h3><p>We have thought about using something more established and complex like <a href=https://arxiv.org/abs/1810.04805>BERT</a> and retraining it on our dataset, however then comes the problem of runtime and memory usage.</p><p>BERT is a huge model. If you thought 150 thousand parameters was a lot, wait till you see BERT’s 110 <em>million</em> parameters. This bad boy takes a few hundred times longer and many times more memory than our current model. While yes BERT may perform really well, we just don’t think it has a place inside of a Chrome extension.</p><p>Our model is decently robust as it is, especially considering the entire model is &lt;2MB and takes less than 200ms to run in browser. For now, we will stick with lightweight models, but we may switch if we find a better match in the future :)</p><h3 id=misclassifications>Misclassifications</h3><p>Of course, this algorithm isn’t perfect. It does have a lot of flaws and weaknesses that we find every day, and we’re working to fix those! If there are any misclassifications that you find in the algorithm, we love to hear about it on our feedback form: <a href=https://forms.gle/ctypb6FmDT9RQqjv6>https://forms.gle/ctypb6FmDT9RQqjv6</a>.</p><h2 id=closing>Closing</h2><p>This NLP model is at the core of reflect. It is this model’s goal to predict whether user intents are valid or not. As a result, we need to make sure this algorithm is accurate, fast, and lightweight. Hopefully, through this blog post, you’ve learned a little about how we went about building a model to fulfill those requirements.</p><p>Learn more about us on our website! ✨ <a href=http://getreflect.app/>http://getreflect.app/</a></p><p>If you have any further questions about reflect or this NLP model, feel free to shoot us an email at <a href=mailto:hello@getreflect.app>hello@getreflect.app</a></p><p><a href=/posts/reflect-nlp-1>Read Part 1 if you haven’t already!</a></p></div></article></main><footer><p>made by jacky zhao, © 2020</p><a href=https://github.com/jackyzha0/blog target=_blank>[source]</a></footer></div></body></html>